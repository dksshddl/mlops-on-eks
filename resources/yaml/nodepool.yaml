apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: gpu
spec:
  template:
    metadata:
      labels:
        managed-by: karpenter
    spec:
      nodeClassRef:
        group: eks.amazonaws.com
        kind: NodeClass
        name: gpu

      requirements:
        - key: eks.amazonaws.com/instance-family
          operator: In
          values: ["p3", "g4dn", "g5"]
        - key: eks.amazonaws.com/instance-gpu-count
          operator: Lt
          values: ["2"]
        - key: "eks.amazonaws.com/instance-cpu"
          operator: In
          values: ["2"]
        - key: "topology.kubernetes.io/zone"
          operator: In
          values: ["ap-northeast-2a", "ap-northeast-2b", "ap-northeast-2c"]
        - key: "kubernetes.io/arch"
          operator: In
          values: ["amd64"]

      taints:
        - key: nvidia.com/gpu
          value: "true"
          effect: "NoSchedule"
      # # Provisioned nodes will have these taints, but pods do not need to tolerate these taints to be provisioned by this
      # # NodePool. These taints are expected to be temporary and some other entity (e.g. a DaemonSet) is responsible for
      # # removing the taint after it has finished initializing the node.
      # startupTaints:
      #   - key: example.com/another-taint
      #     effect: NoSchedule

      # # Note: changing this value in the nodepool will drift the nodeclaims.
      # expireAfter: 720h | Never

      # # The amount of time that a node can be draining before it's forcibly deleted. A node begins draining when a delete call is made against it, starting
      # # its finalization flow. Pods with TerminationGracePeriodSeconds will be deleted preemptively before this terminationGracePeriod ends to give as much time to cleanup as possible.
      # # If your pod's terminationGracePeriodSeconds is larger than this terminationGracePeriod, Karpenter may forcibly delete the pod
      # # before it has its full terminationGracePeriod to cleanup.

      # # Note: changing this value in the nodepool will drift the nodeclaims.
      # terminationGracePeriod: 48h

  # # Disruption section which describes the ways in which Karpenter can disrupt and replace Nodes
  # # Configuration in this section constrains how aggressive Karpenter can be with performing operations
  # # like rolling Nodes due to them hitting their maximum lifetime (expiry) or scaling down nodes to reduce cluster cost
  # disruption:
  #   # Describes which types of Nodes Karpenter should consider for consolidation
  #   # If using 'WhenEmptyOrUnderutilized', Karpenter will consider all nodes for consolidation and attempt to remove or replace Nodes when it discovers that the Node is empty or underutilized and could be changed to reduce cost
  #   # If using `WhenEmpty`, Karpenter will only consider nodes for consolidation that contain no workload pods
  #   consolidationPolicy: WhenEmptyOrUnderutilized | WhenEmpty

  #   # The amount of time Karpenter should wait to consolidate a node after a pod has been added or removed from the node.
  #   # You can choose to disable consolidation entirely by setting the string value 'Never' here
  #   consolidateAfter: 1m | Never # Added to allow additional control over consolidation aggressiveness

  #   # Budgets control the speed Karpenter can scale down nodes.
  #   # Karpenter will respect the minimum of the currently active budgets, and will round up
  #   # when considering percentages. Duration and Schedule must be set together.
  #   budgets:
  #   - nodes: 10%
  #   # On Weekdays during business hours, don't do any deprovisioning.
  #   - schedule: "0 9 * * mon-fri"
  #     duration: 8h
  #     nodes: "0"
  limits:
    cpu: "1000"
    memory: 1000Gi
    nvidia.com/gpu: 4

---
apiVersion: eks.amazonaws.com/v1
kind: NodeClass
metadata:
  name: gpu
spec:
  # Required: Name of IAM Role for Nodes
  role: ${eks-node-role}

  # Required: Subnet selection for node placement
  subnetSelectorTerms:
    - tags:
        type: "private"
        primary: "*"
    # Alternative using direct subnet ID
    # - id: "subnet-0123456789abcdef0"

  # Required: Security group selection for nodes
  securityGroupSelectorTerms:
    # - tags:
    #     Name: "eks-cluster-node-sg"
    # Alternative approaches:
    - id: "sg-0b231232de5f1cf6e"
    # - name: "eks-cluster-node-security-group"

  # Optional: Configure ephemeral storage (shown with default values)
  ephemeralStorage:
    size: "80Gi"    # Range: 1-59000Gi or 1-64000G or 1-58Ti or 1-64T
    iops: 3000      # Range: 3000-16000
    throughput: 125 # Range: 125-1000

  # Optional: Additional EC2 tags
  # tags:
  #   Environment: "production"
  #   Team: "platform"